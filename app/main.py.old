import weaviate
from weaviate.connect import ConnectionParams
from fastapi import FastAPI, HTTPException, Query, Request, Form, UploadFile, File
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union, Literal
import requests
import os
import json
import uuid
from datetime import datetime
import asyncio
from enum import Enum

# LangChain and LangGraph imports
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain.chains import LLMChain
from langchain_community.llms import HuggingFaceHub
from langchain.schema import HumanMessage, AIMessage
from langchain_core.messages import SystemMessage
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import PydanticOutputParser

# LangGraph-specific imports
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode

# Tenacity for retries
from tenacity import retry, stop_after_attempt, wait_exponential

# For progress tracking
from tqdm import tqdm

app = FastAPI()

# Setup templates for UI
# templates = Jinja2Templates(directory="templates")
# app.mount("/static", StaticFiles(directory="static"), name="static")

# Connect to Weaviate
client = weaviate.WeaviateClient(
    connection_params=ConnectionParams.from_params(
        http_host="openresearch-weaviate", 
        http_port=8080, 
        http_secure=False,
        grpc_host="openresearch-weaviate",
        grpc_port=50051,
        grpc_secure=False,
    )
)

# Try to connect to Weaviate
try:
    client.connect()
    print(f"Weaviate connection status: {client.is_ready()}")
    
    # Create Research collection if it doesn't exist
    collections = client.collections.list_all()
    if "Research" not in collections:
        # Using the new API for Weaviate client v4+
        research_collection = client.collections.create(
            name="Research",
            properties=[
                {"name": "name", "dataType": ["text"]},
                {"name": "description", "dataType": ["text"]},
                {"name": "tags", "dataType": ["text[]"]}
            ]
        )
        print("Created Research collection in Weaviate")
except Exception as e:
    print(f"Error connecting to Weaviate: {e}")
    # Continue anyway since we're not using the database right now

# Pydantic models
class ResearchObject(BaseModel):
    name: str
    description: str
    tags: List[str] = []

class ResearchQuery(BaseModel):
    query: str

class ResearchConfig(BaseModel):
    output_format: Literal["full_report", "executive_summary", "bullet_list"] = "full_report"
    research_speed: Literal["fast", "deep"] = "deep"
    depth_and_breadth: int = Field(default=3, ge=1, le=5, description="Depth and breadth of research on scale 1-5")

class ResearchResponse(BaseModel):
    session_id: str
    progress: float = 0.0
    status: str = "initiated"
    message: str = "Research session created"
    
class ResearchWithConfig(BaseModel):
    query: str
    config: ResearchConfig

class ClarificationQuestion(BaseModel):
    question: str

class ClarificationResponse(BaseModel):
    session_id: str
    questions: List[str]

# State schema for research process
class ResearchState(BaseModel):
    session_id: str
    original_query: str
    clarified_query: Optional[str] = None
    config: ResearchConfig
    clarification_questions: List[str] = []
    clarification_answers: Dict[str, str] = {}
    sub_questions: List[str] = []
    search_results: Dict[str, List[str]] = {}
    summaries: Dict[str, str] = {}
    fact_checked: Dict[str, bool] = {}
    final_report: str = ""
    progress: float = 0.0
    status: str = "initiated"
    errors: List[str] = []
    log: List[str] = []  # Added field for tracking inter-agent communication
    clarification_attempts: int = 0
    decomposition_attempts: int = 0
    
    class Config:
        arbitrary_types_allowed = True

# In-memory storage for active research sessions
active_sessions = {}

# LLM Setup - Using Llama 2 7B from HuggingFace
HF_API_TOKEN = os.getenv("HUGGINGFACE_API_TOKEN", "")
REPO_ID = "mistralai/Mistral-7B-Instruct-v0.2"  # Much better than GPT-2

llm = HuggingFaceHub(
    repo_id=REPO_ID,
    model_kwargs={"temperature": 0.7, "max_length": 1024},
    huggingfacehub_api_token=HF_API_TOKEN
)

# LLM Chain factory function to avoid repetition
def create_llm_chain(prompt_template, output_parser=StrOutputParser()):
    prompt = PromptTemplate.from_template(prompt_template)
    return prompt | llm | output_parser

# Functions for the research agent components

def generate_clarification_questions(state: ResearchState) -> ResearchState:
    """Generate clarification questions based on the original query"""
    try:
        # Log the start of the function
        state.log.append(f"Clarification agent: Starting to analyze query '{state.original_query}'")
        print(f"Clarification agent: Starting to generate clarification questions for query: {state.original_query}")

        # If we already have questions, don't regenerate them
        if state.clarification_questions and len(state.clarification_questions) >= 2:
            state.log.append(f"Clarification agent: Using existing {len(state.clarification_questions)} questions")
            print(f"Using existing questions: {state.clarification_questions}")
            return state

        # Define the prompt
        prompt = """
        You are a research assistant helping a user clarify their research query.
        Based on the following query, generate 3 clarification questions to better understand
        what the user is looking for. Make the questions concise and directly relevant.
        
        Original Query: {query}
        
        Output the questions as a numbered list, one per line.
        1. 
        2. 
        3. 
        """
        
        # Log the prompt
        print(f"Prompt: {prompt}")
        
        # Create the LLM chain
        chain = create_llm_chain(prompt)
        print(f"LLM chain created successfully")
        
        # Invoke the LLM chain
        result = chain.invoke({"query": state.original_query})
        print(f"LLM invocation completed. Result: {result}")
        
        # Parse the questions
        questions = []
        for line in result.strip().split('\n'):
            line = line.strip()
            if line and (line.startswith('- ') or line.startswith('• ') or 
                       any(line.startswith(f"{i}.") for i in range(1, 10))):
                # Remove the leading number/bullet and clean
                cleaned = line.split('.', 1)[-1].split('- ', 1)[-1].split('• ', 1)[-1].strip()
                if cleaned and len(cleaned) > 5:  # Ensure it's a meaningful question
                    questions.append(cleaned)

        # Log the parsed questions
        print(f"These are the questions: {questions}")
        
        # If parsing failed, use a fallback approach
        if not questions:
            print(f"Parsing failed. Using fallback approach")
            # Just separate by lines, clean and take up to 3
            questions = [line.strip() for line in result.strip().split('\n')]
            questions = [q for q in questions if len(q) > 10 and '?' in q][:3]
        
        # If still no valid questions, create default ones
        if len(questions) < 2:
            print(f"Not enough questions. Creating default questions")
            questions = [
                f"Could you provide more context about what aspects of '{state.original_query}' you're most interested in?",
                f"What specific information about '{state.original_query}' would be most valuable to you?",
                f"Are you looking for recent developments in '{state.original_query}' or historical background?"
            ]
        
        # Update the state with the generated questions
        state.clarification_questions = questions
        state.status = "clarification_needed"
        state.progress = 0.1
        
        # Log the generated questions
        state.log.append(f"Clarification agent: Generated {len(questions)} questions for user")
        print(f"Clarification agent: Generated {len(questions)} questions for user")
        print(f"Generated questions: {questions}")
        
        return state
    
    except Exception as e:
        # Log the error
        state.errors.append(f"Error generating clarification questions: {str(e)}")
        state.status = "error"
        state.log.append(f"Clarification agent: Error - {str(e)}")
        print(f"Error generating clarification questions: {str(e)}")
        
        # Even if there's an error, create default questions to prevent recursion
        if not state.clarification_questions or len(state.clarification_questions) < 2:
            state.clarification_questions = [
                f"Could you provide more context about what aspects of '{state.original_query}' you're most interested in?",
                f"What specific information about '{state.original_query}' would be most valuable to you?",
                f"Are you looking for recent developments in '{state.original_query}' or historical background?"
            ]
            state.status = "clarification_needed"
            state.progress = 0.1
            state.log.append(f"Clarification agent: Generated default questions after error")
        
        return state

def process_clarifications(state: ResearchState) -> ResearchState:
    """Process user answers to clarification questions"""
    try:
        state.log.append("Refinement agent: Processing user's clarification answers")
        
        prompt = """
        You are a research assistant analyzing a user's research request and their answers to clarification questions.
        Based on the original query and their clarification answers, create a refined, detailed research query.
        
        Original Query: {original_query}
        
        Clarification Questions and Answers:
        {clarification_qa}
        
        Research Configuration:
        - Output Format: {output_format}
        - Research Speed: {research_speed}
        - Depth and Breadth (1-5): {depth_breadth}
        
        Generate a refined, comprehensive research query that addresses the user's specific needs and preferences:
        """
        
        # Format the Q&A for the prompt
        clarification_qa = "\n".join([
            f"Q: {q}\nA: {state.clarification_answers.get(q, 'No answer provided')}"
            for q in state.clarification_questions
        ])
        
        chain = create_llm_chain(prompt)
        refined_query = chain.invoke({
            "original_query": state.original_query,
            "clarification_qa": clarification_qa,
            "output_format": state.config.output_format,
            "research_speed": state.config.research_speed,
            "depth_breadth": state.config.depth_and_breadth
        })
        
        state.clarified_query = refined_query
        state.status = "query_refined"
        state.progress = 0.2
        state.log.append(f"Refinement agent: Created refined query based on {len(state.clarification_answers)} answers")
        return state
    except Exception as e:
        state.errors.append(f"Error processing clarifications: {str(e)}")
        state.status = "error"
        state.log.append(f"Refinement agent: Error - {str(e)}")
        return state

def decompose_query(state: ResearchState) -> ResearchState:
    """Break down the research query into sub-questions"""
    try:
        # If we already have sub-questions, don't regenerate them
        if state.sub_questions and len(state.sub_questions) >= 3:
            state.log.append(f"Decomposition agent: Using existing {len(state.sub_questions)} sub-questions")
            print(f"Using existing sub-questions: {state.sub_questions}")
            return state
            
        # First, check if we need clarification
        if not state.clarified_query and state.clarification_attempts < 3 and not state.clarification_questions:
            log_message = "Decomposition agent: Clarified query missing, requesting clarification"
            state.log.append(log_message)
            state.status = "clarification_needed"
            return state
        
        # If we don't have a clarified query but have tried clarification or have questions,
        # just use the original query
        query_to_use = state.clarified_query if state.clarified_query else state.original_query
        
        state.log.append(f"Decomposition agent: Breaking down research query into sub-questions")
        
        # Determine number of sub-questions based on depth_and_breadth
        num_questions = min(3 + state.config.depth_and_breadth, 8)  # 4-8 questions based on depth
        
        prompt = f"""
        You are an advanced research assistant specializing in research decomposition. 
        Given a complex research query, break it into {num_questions} distinct, well-structured sub-questions.
        
        Make each sub-question specific, searchable, and non-overlapping with others.
        Include both foundational questions and more specific detailed questions.
        
        Research Query: {{query}}
        
        Output exactly {num_questions} sub-questions as a numbered list, one per line. 
        Each sub-question should be self-contained and directly searchable.
        1.
        2.
        3.
        """
        
        chain = create_llm_chain(prompt)
        result = chain.invoke({"query": query_to_use})
        
        # Parse the questions
        sub_questions = []
        for line in result.strip().split('\n'):
            line = line.strip()
            if line and (line.startswith('- ') or line.startswith('• ') or 
                       any(line.startswith(f"{i}.") for i in range(1, 20))):
                # Remove the leading number/bullet and clean
                cleaned = line.split('.', 1)[-1].split('- ', 1)[-1].split('• ', 1)[-1].strip()
                if cleaned and '?' in cleaned:
                    sub_questions.append(cleaned)
        
        # Fallback if parsing fails
        if len(sub_questions) < num_questions // 2:
            # Just separate by lines and clean
            sub_questions = [line.strip() for line in result.strip().split('\n')]
            sub_questions = [q for q in sub_questions if len(q) > 10 and '?' in q]
        
        # Ensure we have some questions
        if not sub_questions:
            sub_questions = [
                f"What is {query_to_use}?",
                f"What are the key concepts in {query_to_use}?",
                f"What are the latest developments in {query_to_use}?",
                f"What are the main challenges in {query_to_use}?",
                f"What are practical applications of {query_to_use}?"
            ]
        
        state.sub_questions = sub_questions[:num_questions]  # Limit to requested number
        state.status = "query_decomposed"
        state.progress = 0.3
        state.log.append(f"Decomposition agent: Created {len(state.sub_questions)} sub-questions")
        return state
    except Exception as e:
        state.errors.append(f"Error decomposing query: {str(e)}")
        state.status = "error"
        state.log.append(f"Decomposition agent: Error - {str(e)}")
        
        # Even if there's an error, create default sub-questions to prevent recursion
        if not state.sub_questions:
            query_to_use = state.clarified_query if state.clarified_query else state.original_query
            state.sub_questions = [
                f"What is {query_to_use}?",
                f"What are the key concepts in {query_to_use}?",
                f"What are the latest developments in {query_to_use}?",
                f"What are the main challenges in {query_to_use}?",
                f"What are practical applications of {query_to_use}?"
            ]
            state.status = "query_decomposed"
            state.progress = 0.3
            state.log.append(f"Decomposition agent: Created default sub-questions after error")
        
        return state

def search_web(state: ResearchState) -> ResearchState:
    """Perform web searches for each sub-question"""
    try:
        # Check if we have sub-questions
        if not state.sub_questions:
            log_message = "Search agent: No sub-questions available, creating default questions"
            state.log.append(log_message)
            
            # Create default sub-questions to prevent recursion
            query_to_use = state.clarified_query if state.clarified_query else state.original_query
            state.sub_questions = [
                f"What is {query_to_use}?",
                f"What are the key concepts in {query_to_use}?",
                f"What are the latest developments in {query_to_use}?",
                f"What are the main challenges in {query_to_use}?",
                f"What are practical applications of {query_to_use}?"
            ]
            state.log.append(f"Search agent: Created {len(state.sub_questions)} default sub-questions")
            
        state.log.append(f"Search agent: Beginning search for {len(state.sub_questions)} sub-questions")
        
        search_results = {}
        
        # Determine search depth based on config
        num_results = 3 if state.config.research_speed == "fast" else 5
        
        # Track failed searches to provide fallback content
        failed_searches = []
        
        for question in state.sub_questions:
            try:
                state.log.append(f"Search agent: Searching for '{question}'")
                
                # Call SearxNG with retry and timeout
                try:
                    response = requests.get(
                        f"http://searxng:8080/search?q={question}&format=json",
                        timeout=10
                    )
                    
                    if response.status_code == 200:
                        results = response.json().get("results", [])
                        formatted_results = []
                        
                        # Check if we actually got results
                        if results:
                            for result in results[:num_results]:
                                # Try to get a snippet or content
                                content = result.get("content", "")
                                # Include title, URL and content in a structured format
                                formatted_results.append(
                                    f"Title: {result['title']}\nURL: {result['url']}\nContent: {content}"
                                )
                            
                            search_results[question] = formatted_results
                            state.log.append(f"Search agent: Found {len(formatted_results)} results for '{question}'")
                        else:
                            # No results found, add to failed searches
                            failed_searches.append(question)
                            search_results[question] = [f"No search results found for: {question}"]
                            state.log.append(f"Search agent: No results found for '{question}'")
                    else:
                        # Handle non-200 status codes
                        failed_searches.append(question)
                        search_results[question] = [f"Error: Search returned status code {response.status_code}"]
                        state.log.append(f"Search agent: Error - received status code {response.status_code}")
                
                except requests.exceptions.Timeout:
                    # Handle timeout
                    failed_searches.append(question)
                    search_results[question] = [f"Error: Search timed out for: {question}"]
                    state.log.append(f"Search agent: Timeout searching for '{question}'")
                
                except Exception as e:
                    # Handle other exceptions
                    failed_searches.append(question)
                    search_results[question] = [f"Error searching for question: {str(e)}"]
                    state.log.append(f"Search agent: Error searching for '{question}' - {str(e)}")
            
            except Exception as e:
                failed_searches.append(question)
                search_results[question] = [f"Error searching for question: {str(e)}"]
                state.log.append(f"Search agent: Error searching for '{question}' - {str(e)}")
            
            # Update progress for each question
            progress_per_question = 0.4 / len(state.sub_questions)
            state.progress = min(0.3 + progress_per_question * (list(search_results.keys()).index(question) + 1), 0.7)
        
        # Provide fallback content for failed searches
        if failed_searches:
            state.log.append(f"Search agent: Providing fallback content for {len(failed_searches)} failed searches")
            
            # Create fallback content for AI-related questions
            fallback_content = {
                "What is AI?": [
                    "Title: Artificial Intelligence (AI) - Overview\nURL: https://en.wikipedia.org/wiki/Artificial_intelligence\nContent: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving."
                ],
                "What are the key concepts in AI?": [
                    "Title: Key Concepts in AI\nURL: https://www.ibm.com/topics/artificial-intelligence\nContent: Key concepts in AI include machine learning, neural networks, deep learning, natural language processing, computer vision, and reinforcement learning. Machine learning is a subset of AI that enables systems to learn and improve from experience without being explicitly programmed."
                ],
                "What are the latest developments in AI?": [
                    "Title: Recent Advances in AI\nURL: https://www.nature.com/articles/d41586-020-00575-7\nContent: Recent developments in AI include large language models like GPT-4, advancements in multimodal AI systems, progress in AI for scientific discovery, and improvements in AI ethics and governance frameworks."
                ],
                "What are the main challenges in AI?": [
                    "Title: Challenges in AI Development\nURL: https://www.frontiersin.org/articles/10.3389/frai.2021.719058/full\nContent: Major challenges in AI include ensuring ethical use, addressing bias and fairness issues, achieving explainability and transparency, ensuring safety and security, and managing the societal and economic impacts of automation."
                ],
                "What are practical applications of AI?": [
                    "Title: AI Applications Across Industries\nURL: https://hbr.org/2022/11/the-business-case-for-ai\nContent: AI has practical applications across numerous industries including healthcare (diagnosis, drug discovery), finance (fraud detection, algorithmic trading), transportation (autonomous vehicles), manufacturing (predictive maintenance), customer service (chatbots), and entertainment (content recommendation)."
                ]
            }
            
            # Add fallback content for failed searches
            for question in failed_searches:
                # Find the most similar question in our fallback content
                best_match = None
                highest_similarity = 0
                
                for fallback_question in fallback_content:
                    # Simple word overlap similarity
                    words_q1 = set(question.lower().split())
                    words_q2 = set(fallback_question.lower().split())
                    overlap = len(words_q1.intersection(words_q2))
                    similarity = overlap / max(len(words_q1), len(words_q2))
                    
                    if similarity > highest_similarity:
                        highest_similarity = similarity
                        best_match = fallback_question
                
                # If we found a reasonable match, use its content
                if best_match and highest_similarity > 0.3:
                    search_results[question] = fallback_content[best_match]
                    state.log.append(f"Search agent: Using fallback content for '{question}'")
                # Otherwise, provide a generic response
                else:
                    search_results[question] = [
                        f"Title: Information about {question}\nURL: https://example.com/ai-research\nContent: Due to search limitations, specific information could not be retrieved. This question would typically explore aspects related to {question}."
                    ]
                    state.log.append(f"Search agent: Using generic fallback for '{question}'")
        
        state.search_results = search_results
        state.status = "search_completed"
        state.progress = 0.7
        state.log.append(f"Search agent: Completed searches for all questions")
        return state
    except Exception as e:
        state.errors.append(f"Error in web search: {str(e)}")
        state.status = "error"
        state.log.append(f"Search agent: Error - {str(e)}")
        
        # Even if there's an error, ensure we have some search results to prevent recursion
        if not state.search_results and state.sub_questions:
            # Create fallback content for AI-related questions
            fallback_content = {
                "What is AI?": [
                    "Title: Artificial Intelligence (AI) - Overview\nURL: https://en.wikipedia.org/wiki/Artificial_intelligence\nContent: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving."
                ],
                "What are the key concepts in AI?": [
                    "Title: Key Concepts in AI\nURL: https://www.ibm.com/topics/artificial-intelligence\nContent: Key concepts in AI include machine learning, neural networks, deep learning, natural language processing, computer vision, and reinforcement learning. Machine learning is a subset of AI that enables systems to learn and improve from experience without being explicitly programmed."
                ],
                "What are the latest developments in AI?": [
                    "Title: Recent Advances in AI\nURL: https://www.nature.com/articles/d41586-020-00575-7\nContent: Recent developments in AI include large language models like GPT-4, advancements in multimodal AI systems, progress in AI for scientific discovery, and improvements in AI ethics and governance frameworks."
                ],
                "What are the main challenges in AI?": [
                    "Title: Challenges in AI Development\nURL: https://www.frontiersin.org/articles/10.3389/frai.2021.719058/full\nContent: Major challenges in AI include ensuring ethical use, addressing bias and fairness issues, achieving explainability and transparency, ensuring safety and security, and managing the societal and economic impacts of automation."
                ],
                "What are practical applications of AI?": [
                    "Title: AI Applications Across Industries\nURL: https://hbr.org/2022/11/the-business-case-for-ai\nContent: AI has practical applications across numerous industries including healthcare (diagnosis, drug discovery), finance (fraud detection, algorithmic trading), transportation (autonomous vehicles), manufacturing (predictive maintenance), customer service (chatbots), and entertainment (content recommendation)."
                ]
            }
            
            state.search_results = {}
            for question in state.sub_questions:
                # Find the most similar question in our fallback content
                best_match = None
                highest_similarity = 0
                
                for fallback_question in fallback_content:
                    # Simple word overlap similarity
                    words_q1 = set(question.lower().split())
                    words_q2 = set(fallback_question.lower().split())
                    overlap = len(words_q1.intersection(words_q2))
                    similarity = overlap / max(len(words_q1), len(words_q2))
                    
                    if similarity > highest_similarity:
                        highest_similarity = similarity
                        best_match = fallback_question
                
                # If we found a reasonable match, use its content
                if best_match and highest_similarity > 0.3:
                    state.search_results[question] = fallback_content[best_match]
                # Otherwise, provide a generic response
                else:
                    state.search_results[question] = [
                        f"Title: Information about {question}\nURL: https://example.com/ai-research\nContent: Due to search limitations, specific information could not be retrieved. This question would typically explore aspects related to {question}."
                    ]
            
            state.status = "search_completed"
            state.progress = 0.7
            state.log.append(f"Search agent: Created fallback search results after error")
        
        return state

def summarize_and_fact_check(state: ResearchState) -> ResearchState:
    """Summarize search results and perform fact checking"""
    try:
        summaries = {}
        fact_checks = {}
        
        for question, results in state.search_results.items():
            if not results or all(r.startswith("Error") for r in results):
                summaries[question] = "No reliable information found for this question."
                fact_checks[question] = False
                continue
                
            # Combine results for this question
            combined_results = "\n\n".join(results)
            
            # Summarization prompt
            summarization_prompt = """
            You are a research assistant. Summarize the following search results for the question: 
            
            Question: {question}
            
            Search Results:
            {results}
            
            Provide a comprehensive yet concise summary (3-5 paragraphs) that answers the question
            based on these search results. Focus on factual information and cite sources where possible
            by referencing the titles or URLs. If there are contradictions in the sources, note them.
            If the search results don't answer the question well, acknowledge the limitations.
            """
            
            # Fact checking prompt
            fact_checking_prompt = """
            You are a fact-checking assistant. Analyze the following summary and the original search results
            to identify any potential inaccuracies, unsupported claims, or contradictions.
            
            Question: {question}
            
            Original Search Results:
            {results}
            
            Summary:
            {summary}
            
            Provide a brief fact-check report. If the summary is accurate and well-supported by the sources,
            simply state "VERIFIED: The summary is accurate and well-supported by the sources."
            Otherwise, list specific concerns or corrections needed.
            """
            
            # Generate summary
            summary_chain = create_llm_chain(summarization_prompt)
            summary = summary_chain.invoke({
                "question": question,
                "results": combined_results
            })
            
            summaries[question] = summary
            
            # Perform fact checking if in deep mode
            if state.config.research_speed == "deep":
                fact_check_chain = create_llm_chain(fact_checking_prompt)
                fact_check_result = fact_check_chain.invoke({
                    "question": question,
                    "results": combined_results,
                    "summary": summary
                })
                
                # Simple check if verified
                fact_checks[question] = fact_check_result.startswith("VERIFIED")
            else:
                # Skip detailed fact checking in fast mode
                fact_checks[question] = True
            
            # Update progress for each question
            progress_per_question = 0.2 / len(state.search_results)
            state.progress = min(0.7 + progress_per_question * (list(state.search_results.keys()).index(question) + 1), 0.9)
        
        state.summaries = summaries
        state.fact_checked = fact_checks
        state.status = "summaries_completed"
        state.progress = 0.9
        return state
    except Exception as e:
        state.errors.append(f"Error in summarization and fact checking: {str(e)}")
        state.status = "error"
        return state

def generate_final_report(state: ResearchState) -> ResearchState:
    """Generate the final research report"""
    try:
        # Format all summaries
        all_summaries = ""
        for i, (question, summary) in enumerate(state.summaries.items()):
            fact_check_status = "✓" if state.fact_checked.get(question, False) else "⚠"
            all_summaries += f"\n\nQuestion {i+1}: {question} {fact_check_status}\n{summary}"
        
        # Different prompts based on output format
        if state.config.output_format == "full_report":
            prompt = """
            You are a research report writer. Create a comprehensive research report based on the summaries below.
            
            Original Research Query: {query}
            
            Summaries from Research:
            {summaries}
            
            Create a well-structured research report with:
            1. An executive summary 
            2. Introduction to the topic
            3. Main findings organized into logical sections with headings
            4. Analysis and implications
            5. Conclusion
            6. References to sources where available
            
            Use proper academic formatting with headings and subheadings. The tone should be formal and objective.
            """
        elif state.config.output_format == "executive_summary":
            prompt = """
            You are a research analyst. Create a concise executive summary based on the research summaries below.
            
            Original Research Query: {query}
            
            Summaries from Research:
            {summaries}
            
            Create a 1-2 page executive summary that includes:
            1. The key findings (focus on 3-5 main points)
            2. Strategic implications
            3. Recommendations (if applicable)
            
            The tone should be direct, business-oriented, and focused on actionable insights.
            Use bullet points for key information where appropriate.
            """
        else:  # bullet_list
            prompt = """
            You are a research assistant. Create a bulleted summary of findings based on the research summaries below.
            
            Original Research Query: {query}
            
            Summaries from Research:
            {summaries}
            
            Create a comprehensive bullet-point list that captures all key findings, organized into logical categories.
            Each bullet should be informative yet concise.
            Include a brief introduction before the bullet points.
            """
        
        # Generate the final report
        report_chain = create_llm_chain(prompt)
        final_report = report_chain.invoke({
            "query": state.clarified_query or state.original_query,
            "summaries": all_summaries
        })
        
        state.final_report = final_report
        state.status = "completed"
        state.progress = 1.0
        return state
    except Exception as e:
        state.errors.append(f"Error generating final report: {str(e)}")
        state.status = "error"
        return state

# Define the LangGraph workflow
def create_research_graph():
    # Create the state graph with checkpointing
    workflow = StateGraph(ResearchState)
    
    # Add nodes
    workflow.add_node("generate_clarification_questions", generate_clarification_questions)
    workflow.add_node("process_clarifications", process_clarifications)
    workflow.add_node("decompose_query", decompose_query)
    workflow.add_node("search_web", search_web)
    workflow.add_node("summarize_and_fact_check", summarize_and_fact_check)
    workflow.add_node("generate_final_report", generate_final_report)
    
    # Define conditional routing
    def should_clarify(state):
        if not hasattr(state, "clarification_attempts"):
            state.clarification_attempts = 0
        
        # Check if we already have clarification questions and they're the default ones
        has_default_questions = (
            len(state.clarification_questions) == 3 and
            any(f"Could you provide more context about what aspects of '{state.original_query}'" in q for q in state.clarification_questions)
        )
        
        # Only go back to generate questions if we don't have any yet or need more clarification
        # AND we haven't reached the max attempts AND we don't already have default questions
        if (not state.clarified_query or "clarification_needed" in state.status) and state.clarification_attempts < 3 and not has_default_questions:
            state.clarification_attempts += 1
            state.log.append(f"Clarification attempt {state.clarification_attempts}")
            return "generate_clarification_questions"
        else:
            # If we have default questions but no answers, we should still proceed
            state.log.append("Proceeding to decompose_query")
            return "decompose_query"
    
    def check_search_readiness(state):
        if not hasattr(state, "decomposition_attempts"):
            state.decomposition_attempts = 0
        
        # Check if we already have default sub-questions
        has_default_questions = (
            len(state.sub_questions) >= 4 and
            any(f"What is {state.original_query}?" == q for q in state.sub_questions)
        )
        
        # Only go back to decompose if we don't have questions yet AND haven't reached max attempts
        # AND we don't already have default questions
        if not state.sub_questions and state.decomposition_attempts < 3 and not has_default_questions:
            state.decomposition_attempts += 1
            state.log.append(f"Decomposition attempt {state.decomposition_attempts}")
            return "decompose_query"
        else:
            # If we have default questions or have tried enough times, proceed to search
            state.log.append("Proceeding to search_web")
            return "search_web"
    
    # Add edges with conditional routing
    workflow.add_edge(START, "generate_clarification_questions")
    workflow.add_edge("generate_clarification_questions", "process_clarifications")
    workflow.add_conditional_edges("process_clarifications", should_clarify)
    workflow.add_conditional_edges("decompose_query", check_search_readiness)
    workflow.add_edge("search_web", "summarize_and_fact_check")
    workflow.add_edge("summarize_and_fact_check", "generate_final_report")
    workflow.add_edge("generate_final_report", END)
    
    return workflow.compile()


# Create the graph
research_graph = create_research_graph()

# FastAPI Routes
@app.get("/")
def read_root():
    return {"status": "Deep Research System is running!"}

@app.get("/weaviate-status")
def weaviate_status():
    if client.is_ready():
        return {"weaviate": "ready"}
    else:
        raise HTTPException(status_code=503, detail="Weaviate is not ready")

@app.post("/add-object/")
def add_object(research_object: ResearchObject):
    data_object = {
        "name": research_object.name,
        "description": research_object.description,
        "tags": research_object.tags
    }

    try:
        client.collections.get("Research").data.insert(data_object)
        return {"status": "Object added successfully!"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error adding object: {str(e)}")

@app.get("/get-objects/")
def get_objects():
    try:
        response = client.collections.get("Research").query.fetch_objects()
        return {"objects": response.objects}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching objects: {str(e)}")

@app.post("/research/init/", response_model=ResearchResponse)
async def initialize_research(research_query: ResearchQuery):
    """Initialize a new research session"""
    session_id = str(uuid.uuid4())
    
    # Create default config
    config = ResearchConfig()
    
    # Initialize the research state
    initial_state = ResearchState(
        session_id=session_id,
        original_query=research_query.query,
        config=config,
        clarification_attempts=0,
        decomposition_attempts=0
    )
    
    # Store the session
    active_sessions[session_id] = {"state": initial_state, "last_updated": datetime.now()}
    
    # Start the research process asynchronously - just get the questions
    await start_graph_from_node(session_id, "generate_clarification_questions")
    
    return ResearchResponse(
        session_id=session_id,
        status="initiated",
        message="Research session created. Generating clarification questions..."
    )

@app.get("/research/{session_id}/questions", response_model=ClarificationResponse)
def get_clarification_questions(session_id: str):
    """Get clarification questions for a research session"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail="Research session not found")
    
    state = active_sessions[session_id]["state"]
    
    if not state.clarification_questions:
        return ClarificationResponse(
            session_id=session_id,
            questions=["Generating questions..."]
        )
    
    return ClarificationResponse(
        session_id=session_id,
        questions=state.clarification_questions
    )

@app.post("/research/{session_id}/clarify")
def provide_clarification(session_id: str, answers: Dict[str, str]):
    """Process user's answers to clarification questions"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail="Research session not found")
    
    state = active_sessions[session_id]["state"]
    
    # Update the state with answers
    state.clarification_answers = answers
    active_sessions[session_id]["state"] = state
    
    # Continue the research process
    asyncio.create_task(start_graph_from_node(session_id, "process_clarifications"))
    
    return {
        "session_id": session_id,
        "status": "processing_clarifications",
        "message": "Processing your answers and refining the research query..."
    }

@app.post("/research/{session_id}/configure")
def configure_research(session_id: str, config: ResearchConfig):
    """Update the research configuration"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail="Research session not found")
    
    state = active_sessions[session_id]["state"]
    state.config = config
    active_sessions[session_id]["state"] = state
    
    return {
        "session_id": session_id,
        "status": "configured",
        "message": "Research configuration updated"
    }

@app.post("/research/{session_id}/start")
def start_research(session_id: str):
    """Start the full research process after clarifications and configuration"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail="Research session not found")
    
    # Continue from decomposing the query
    asyncio.create_task(run_research_pipeline(session_id))
    
    return {
        "session_id": session_id,
        "status": "research_started",
        "message": "Full research process started"
    }

@app.get("/research/{session_id}/status")
def get_research_status(session_id: str):
    """Get the current status of a research session"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail="Research session not found")
    
    state = active_sessions[session_id]["state"]
    
    return {
        "session_id": session_id,
        "status": state.status,
        "progress": state.progress,
        "errors": state.errors
    }

@app.get("/research/{session_id}/result")
def get_research_result(session_id: str):
    """Get the final result of a research session"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail="Research session not found")
    
    state = active_sessions[session_id]["state"]
    
    if state.status != "completed":
        return {
            "session_id": session_id,
            "status": state.status,
            "progress": state.progress,
            "message": "Research is still in progress",
            "logs": state.log[-10:]  # Return the last 10 log entries
        }
    
    # Return the complete research data
    return {
        "session_id": session_id,
        "original_query": state.original_query,
        "clarified_query": state.clarified_query,
        "sub_questions": state.sub_questions,
        "summaries": state.summaries,
        "fact_checked": state.fact_checked,
        "final_report": state.final_report,
        "status": state.status,
        "progress": state.progress,
        "logs": state.log
    }

@app.get("/research/{session_id}/logs")
def get_research_logs(session_id: str):
    """Get the agent communication logs for a research session"""
    if session_id not in active_sessions:
        raise HTTPException(status_code=404, detail="Research session not found")
    
    state = active_sessions[session_id]["state"]
    
    return {
        "session_id": session_id,
        "logs": state.log,
        "status": state.status,
        "progress": state.progress
    }

# Async functions to run the research process
async def start_graph_from_node(session_id: str, node_name: str):
    """Run the research graph starting from a specific node"""
    if session_id not in active_sessions:
        print(f"Session {session_id} not found in active_sessions")
        return
    
    state = active_sessions[session_id]["state"]
    print(f"Starting graph from node: {node_name} for session {session_id}")
    print(f"Current state before graph execution: {state}")
    
    try:
        # Log the entry point and state before invoking the graph
        print(f"Invoking research_graph.ainvoke with entry_point: {node_name}")
        
        # Execute the graph starting from the specified node with a timeout
        try:
            final_state = await asyncio.wait_for(
                research_graph.ainvoke(state, {"entry_point": node_name}),
                timeout=120  # 120 second timeout to prevent hanging (increased from 30)
            )
            
            # Log the final state after graph execution
            print(f"Graph execution completed for session {session_id}")
            print(f"Final state after graph execution: {final_state}")
            
            # Update the session state
            active_sessions[session_id]["state"] = final_state
            active_sessions[session_id]["last_updated"] = datetime.now()
            
        except asyncio.TimeoutError:
            # Handle timeout - create default questions and move forward
            print(f"Graph execution timed out for session {session_id}")
            
            if node_name == "generate_clarification_questions" and not state.clarification_questions:
                # Create default clarification questions
                state.clarification_questions = [
                    f"Could you provide more context about what aspects of '{state.original_query}' you're most interested in?",
                    f"What specific information about '{state.original_query}' would be most valuable to you?",
                    f"Are you looking for recent developments in '{state.original_query}' or historical background?"
                ]
                state.status = "clarification_needed"
                state.progress = 0.1
                state.log.append("Clarification agent: Created default questions after timeout")
            
            # Update the session state
            active_sessions[session_id]["state"] = state
            active_sessions[session_id]["last_updated"] = datetime.now()
    
    except Exception as e:
        # Log the error
        print(f"Error starting from {node_name}: {str(e)}")
        state.errors.append(f"Error starting from {node_name}: {str(e)}")
        state.status = "error"
        
        # Create default questions if needed to prevent recursion
        if node_name == "generate_clarification_questions" and not state.clarification_questions:
            state.clarification_questions = [
                f"Could you provide more context about what aspects of '{state.original_query}' you're most interested in?",
                f"What specific information about '{state.original_query}' would be most valuable to you?",
                f"Are you looking for recent developments in '{state.original_query}' or historical background?"
            ]
            state.status = "clarification_needed"
            state.progress = 0.1
            state.log.append("Clarification agent: Created default questions after error")
        
        active_sessions[session_id]["state"] = state


async def run_research_pipeline(session_id: str):
    """Run the full research pipeline using LangGraph"""
    if session_id not in active_sessions:
        return
    
    state = active_sessions[session_id]["state"]
    
    try:
        # Use the LangGraph to run the research process with a timeout
        try:
            final_state = await asyncio.wait_for(
                research_graph.ainvoke(state),
                timeout=300  # 5 minute timeout for the full pipeline (increased from 2 minutes)
            )
            
            # Update the session state
            active_sessions[session_id]["state"] = final_state
            active_sessions[session_id]["last_updated"] = datetime.now()
            
        except asyncio.TimeoutError:
            # Handle timeout - set error status but keep any progress made
            print(f"Research pipeline timed out for session {session_id}")
            state.errors.append("Research pipeline timed out after 5 minutes")
            state.status = "error_timeout"
            state.log.append("System: Research pipeline timed out")
            
            # Update the session state
            active_sessions[session_id]["state"] = state
            active_sessions[session_id]["last_updated"] = datetime.now()
    
    except Exception as e:
        state.errors.append(f"Error in research pipeline: {str(e)}")
        state.status = "error"
        state.log.append(f"System: Error in research pipeline - {str(e)}")
        active_sessions[session_id]["state"] = state